{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport os\nfrom tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D,Flatten, Dropout\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.layers import BatchNormalization, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nimport seaborn as sn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_generators():\n    train_dir = '/kaggle/input/private-datasetrar/227x227/train'\n    valid_dir = '/kaggle/input/private-datasetrar/227x227/valid'\n    test_dir = '/kaggle/input/private-datasetrar/227x227/test'\n\n    batch_size = 32\n    img_height, img_width = (227, 227)\n\n    train_datagen = ImageDataGenerator(\n        preprocessing_function=preprocess_input,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        validation_split=0.4\n    )\n    train_generator = train_datagen.flow_from_directory(\n        directory=train_dir,\n        target_size=(img_height, img_width),\n        batch_size=batch_size,\n        class_mode='categorical',\n        subset='training'\n    )\n    valid_generator = train_datagen.flow_from_directory(\n        directory=valid_dir,\n        target_size=(img_height, img_width),\n        batch_size=1,\n        class_mode='categorical',\n        subset='validation'\n    )\n    test_generator = train_datagen.flow_from_directory(\n        directory=test_dir,\n        target_size=(img_height, img_width),\n        batch_size=1,\n        class_mode='categorical',\n        subset='validation'\n    )\n    \n    return train_generator, valid_generator, test_generator\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def AlexNet(input_shape, num_classes):\n    input_layer = Input(shape=input_shape)\n\n    # First convolutional layer\n    X = Conv2D(filters=96, kernel_size=(11, 11), strides=4, activation='relu', padding=\"same\")(input_layer)\n    X = BatchNormalization()(X)\n    X = MaxPooling2D(pool_size=(3, 3), strides=2)(X)\n\n    # Second convolutional layer\n    X = Conv2D(filters=256, kernel_size=(5, 5), activation='relu', padding=\"same\")(X)\n    X = BatchNormalization()(X)\n    X = MaxPooling2D(pool_size=(3, 3), strides=2)(X)\n\n    # Third convolutional layer\n    X = Conv2D(filters=384, kernel_size=(3, 3), strides=1, activation='relu', padding=\"same\")(X)\n    X = BatchNormalization()(X)\n\n    # Fourth convolutional layer\n    X = Conv2D(filters=384, kernel_size=(3, 3), strides=1, activation='relu', padding=\"same\")(X)\n    X = BatchNormalization()(X)\n\n    # Fifth convolutional layer\n    X = Conv2D(filters=256, kernel_size=(3, 3), strides=1, activation='relu', padding=\"same\")(X)\n    X = BatchNormalization()(X)\n    X = MaxPooling2D(pool_size=(3, 3), strides=2)(X)\n\n    # Flattening\n    X = Flatten()(X)\n\n    # First fully connected layer\n    X = Dense(4096, activation='relu')(X)\n    X = Dropout(0.5)(X)\n\n    # Second fully connected layer\n    X = Dense(4096, activation='relu')(X)\n    X = Dropout(0.5)(X)\n\n    # Output layer\n    predictions = Dense(num_classes, activation='softmax')(X)\n\n    # Create the model\n    model = Model(inputs=input_layer, outputs=predictions, name=\"AlexNet\")\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compile_and_fit_model(model, train_generator, valid_generator):\n    EPOCHS = 50\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    checkpoint_filepath_best = '/kaggle/working/Alexnet/50epoch.{epoch:02d}-{val_loss:.2f}.h5'\n    checkpoint_best = ModelCheckpoint(\n        filepath=checkpoint_filepath_best,\n        save_best_only=True,\n        save_weights_only=False,\n        monitor='val_accuracy',\n        mode='max',\n        verbose=1\n    )\n    history = model.fit(\n        train_generator,\n        epochs=EPOCHS,\n        validation_data=valid_generator,\n        callbacks=[checkpoint_best]\n    )\n    return history\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_training_history(history):\n    fig, axs = plt.subplots(2, 1, figsize=(15, 10))\n\n    # Plot Training Accuracy and Validation Accuracy\n    axs[0].plot(history.history['accuracy'])\n    axs[0].plot(history.history['val_accuracy'])\n    axs[0].set_title('AlexNet Training Accuracy VS Validation Accuracy')\n    axs[0].set_xlabel('Epochs')\n    axs[0].set_ylabel('Accuracy')\n    axs[0].legend(['Training', 'Validation'])\n\n    # Plot Training Loss and Validation Loss\n    axs[1].plot(history.history['loss'])\n    axs[1].plot(history.history['val_loss'])\n    axs[1].set_title('AlexNet Training Loss VS Validation Loss')\n    axs[1].set_xlabel('Epochs')\n    axs[1].set_ylabel('Loss')\n    axs[1].legend(['Training', 'Validation'])\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_best_model():\n    saved_models_dir = '/kaggle/working/Alexnet/'\n    saved_model_files = glob.glob(os.path.join(saved_models_dir, '*.h5'))\n    saved_model_files.sort()\n    best_model_file = saved_model_files[-1]\n    best_model = tf.keras.models.load_model(best_model_file)\n\n    best_model.save('/kaggle/working/Alexnet/Best_AlexNet.h5')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_models(test_generator, train_generator):\n    saved_model_filepaths = glob.glob('/kaggle/working/Alexnet/*.h5')\n\n    for model_filepath in saved_model_filepaths:\n        print(\"Model file path: {}\".format(model_filepath))\n\n        model = tf.keras.models.load_model(model_filepath)\n\n        filenames = test_generator.filenames\n        nb_samples = len(filenames)\n\n        y_prob = []\n        y_act = []\n\n        test_generator.reset()\n        for _ in range(nb_samples):\n            X_test, Y_test = test_generator.next()\n            y_prob.append(model.predict(X_test))\n            y_act.append(Y_test)\n\n        predicted_class = [list(train_generator.class_indices.keys())[i.argmax()] for i in y_prob]\n        actual_class = [list(train_generator.class_indices.keys())[i.argmax()] for i in y_act]\n\n        out_df = pd.DataFrame(np.vstack([predicted_class, actual_class]).T, columns=['predicted_class', 'actual_class'])\n\n        confusion_matrix = pd.crosstab(out_df['actual_class'], out_df['predicted_class'], rownames=['Actual'], colnames=['Predicted'])\n\n        sn.heatmap(confusion_matrix, cmap='Reds', annot=True, fmt='d')\n        plt.title('Confusion Matrix for Model: {}'.format(model_filepath))\n        plt.show()\n\n        print('Test accuracy: {}'.format((np.diagonal(confusion_matrix).sum() / confusion_matrix.sum().sum() * 100)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    train_generator, valid_generator, test_generator = get_data_generators()\n    model = AlexNet(train_generator.image_shape, train_generator.num_classes)\n    history = compile_and_fit_model(model, train_generator, valid_generator)\n    plot_training_history(history)\n    save_best_model()\n    evaluate_models(test_generator, train_generator)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}